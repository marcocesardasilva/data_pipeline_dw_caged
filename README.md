# Data Pipeline para Data Warehouse do Novo Caged

Este projeto tem o objetivo de desenvolver um pipeline de dados para a construção de um Data Warehouse para análise de dados do Novo Caged.

A arquitetura definida foi on premise e utilizará as seguintes ferramentas:

* StarUML: modelagem dimensional e modelo físico de dados
* Docker Desktop: foi utilizado para subir o banco de dados sem a necessidade de instalação;
* PostgreSQL: foi o banco de dados relacional utilizado para persistir os dados do Data Warehouse;
* Dbeaver: Conexão com o banco de dados e manipulação dos mesmos com SQL;
* Pentaho Data Integration (Kettle): foi a ferramenta escolhida para realizar a extração, transformação e carga dos dados (ETL);
* Agendador de tarefas do windows: Automatizar o start da execução do processo mensalmente;
* PowerBi Desktop: Visualização e análise dos dados
#
## Instalação do StarUML

* Baixar o instalador em: https://staruml.io/download
#
## Instalação do Docker Desktop

* Baixar o instalador em: https://www.docker.com/products/docker-desktop/
#
## Criação do container para o banco de dados PostgreSQL


#
## Instalação do Dbeaver e conecção com o banco de dados

* Baixar o instalador em: https://dbeaver.com/download/lite/
#
## Criação das tabelas no banco de dados


#
## Instação e configuração do Pentaho Data Integration (Kettle)

* Baixar o instalador em: https://sourceforge.net/projects/pentaho/
#
## Configuração dos jobs e transformations e testes no PDI


#
## Agendamento da execução periódica com o agendador de tarefas do Windos


#
## Instalação do PowerBi Desktop

* Baixar o instalador em: https://powerbi.microsoft.com/pt-br/desktop/
#
## Conecção do PowerBi com o banco de dados PostgreSQL



#